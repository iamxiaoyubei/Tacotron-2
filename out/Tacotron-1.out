Checkpoint path: logs-Tacotron/taco_pretrained/tacotron_model.ckpt
Loading training data from: training_data/train.txt
Using model: Tacotron
Hyperparameters:
  GL_on_GPU: True
  NN_init: True
  NN_scaler: 0.3
  allow_clipping_in_normalization: True
  attention_dim: 128
  attention_filters: 32
  attention_kernel: (31,)
  attention_win_size: 7
  batch_norm_position: after
  cbhg_conv_channels: 128
  cbhg_highway_units: 128
  cbhg_highwaynet_layers: 4
  cbhg_kernels: 8
  cbhg_pool_size: 2
  cbhg_projection: 256
  cbhg_projection_kernel_size: 3
  cbhg_rnn_units: 128
  cdf_loss: False
  cin_channels: 80
  cleaners: english_cleaners
  clip_for_wavenet: True
  clip_mels_length: True
  clip_outputs: True
  cross_entropy_pos_weight: 1
  cumulative_weights: True
  decoder_layers: 2
  decoder_lstm_units: 1024
  embedding_dim: 512
  enc_conv_channels: 512
  enc_conv_kernel_size: (5,)
  enc_conv_num_layers: 3
  encoder_lstm_units: 256
  fmax: 7600
  fmin: 55
  frame_shift_ms: None
  freq_axis_kernel_size: 3
  gate_channels: 256
  gin_channels: -1
  griffin_lim_iters: 60
  hop_size: 275
  input_type: raw
  kernel_size: 3
  layers: 20
  leaky_alpha: 0.4
  legacy: True
  log_scale_min: -32.23619130191664
  log_scale_min_gauss: -16.11809565095832
  lower_bound_decay: 0.1
  magnitude_power: 2.0
  mask_decoder: False
  mask_encoder: True
  max_abs_value: 4.0
  max_iters: 10000
  max_mel_frames: 900
  max_time_sec: None
  max_time_steps: 11000
  min_level_db: -100
  n_fft: 2048
  n_speakers: 5
  normalize_for_wavenet: True
  num_freq: 1025
  num_mels: 80
  out_channels: 2
  outputs_per_step: 1
  postnet_channels: 512
  postnet_kernel_size: (5,)
  postnet_num_layers: 5
  power: 1.5
  predict_linear: True
  preemphasis: 0.97
  preemphasize: True
  prenet_layers: [256, 256]
  quantize_channels: 65536
  ref_level_db: 20
  rescale: True
  rescaling_max: 0.999
  residual_channels: 128
  residual_legacy: True
  sample_rate: 22050
  signal_normalization: True
  silence_threshold: 2
  skip_out_channels: 128
  smoothing: False
  speakers: ['speaker0', 'speaker1', 'speaker2', 'speaker3', 'speaker4']
  speakers_path: None
  split_on_cpu: True
  stacks: 2
  stop_at_any: True
  symmetric_mels: True
  synthesis_constraint: False
  synthesis_constraint_type: window
  tacotron_adam_beta1: 0.9
  tacotron_adam_beta2: 0.999
  tacotron_adam_epsilon: 1e-06
  tacotron_batch_size: 32
  tacotron_clip_gradients: True
  tacotron_data_random_state: 1234
  tacotron_decay_learning_rate: True
  tacotron_decay_rate: 0.5
  tacotron_decay_steps: 18000
  tacotron_dropout_rate: 0.5
  tacotron_final_learning_rate: 0.0001
  tacotron_fine_tuning: False
  tacotron_initial_learning_rate: 0.001
  tacotron_natural_eval: False
  tacotron_num_gpus: 1
  tacotron_random_seed: 5339
  tacotron_reg_weight: 1e-06
  tacotron_scale_regularization: False
  tacotron_start_decay: 40000
  tacotron_swap_with_cpu: False
  tacotron_synthesis_batch_size: 1
  tacotron_teacher_forcing_decay_alpha: None
  tacotron_teacher_forcing_decay_steps: 40000
  tacotron_teacher_forcing_final_ratio: 0.0
  tacotron_teacher_forcing_init_ratio: 1.0
  tacotron_teacher_forcing_mode: constant
  tacotron_teacher_forcing_ratio: 1.0
  tacotron_teacher_forcing_start_decay: 10000
  tacotron_test_batches: None
  tacotron_test_size: 0.05
  tacotron_zoneout_rate: 0.1
  train_with_GTA: True
  trim_fft_size: 2048
  trim_hop_size: 512
  trim_silence: True
  trim_top_db: 40
  upsample_activation: Relu
  upsample_scales: [11, 25]
  upsample_type: SubPixel
  use_bias: True
  use_lws: False
  use_speaker_embedding: True
  wavenet_adam_beta1: 0.9
  wavenet_adam_beta2: 0.999
  wavenet_adam_epsilon: 1e-06
  wavenet_batch_size: 8
  wavenet_clip_gradients: True
  wavenet_data_random_state: 1234
  wavenet_debug_mels: ['training_data/mels/mel-LJ001-0008.npy']
  wavenet_debug_wavs: ['training_data/audio/audio-LJ001-0008.npy']
  wavenet_decay_rate: 0.5
  wavenet_decay_steps: 200000
  wavenet_dropout: 0.05
  wavenet_ema_decay: 0.9999
  wavenet_gradient_max_norm: 100.0
  wavenet_gradient_max_value: 5.0
  wavenet_init_scale: 1.0
  wavenet_learning_rate: 0.001
  wavenet_lr_schedule: exponential
  wavenet_natural_eval: False
  wavenet_num_gpus: 1
  wavenet_pad_sides: 1
  wavenet_random_seed: 5339
  wavenet_swap_with_cpu: False
  wavenet_synth_debug: False
  wavenet_synthesis_batch_size: 20
  wavenet_test_batches: 1
  wavenet_test_size: None
  wavenet_warmup: 4000.0
  wavenet_weight_normalization: False
  win_size: 1100
Loaded metadata for 13100 examples (23.76 hours)
initialisation done /gpu:0
Initialized Tacotron model. Dimensions (? = dynamic shape): 
  Train mode:               True
  Eval mode:                False
  GTA mode:                 False
  Synthesis mode:           False
  Input:                    (?, ?)
  device:                   0
  embedding:                (?, ?, 512)
  enc conv out:             (?, ?, 512)
  encoder out:              (?, ?, 512)
  decoder out:              (?, ?, 80)
  residual out:             (?, ?, 512)
  projected residual out:   (?, ?, 80)
  mel out:                  (?, ?, 80)
  linear out:               (?, ?, 1025)
  <stop_token> out:         (?, ?)
  Tacotron Parameters       29.016 Million.
initialisation done /gpu:0
Initialized Tacotron model. Dimensions (? = dynamic shape): 
  Train mode:               False
  Eval mode:                True
  GTA mode:                 False
  Synthesis mode:           False
  Input:                    (?, ?)
  device:                   0
  embedding:                (?, ?, 512)
  enc conv out:             (?, ?, 512)
  encoder out:              (?, ?, 512)
  decoder out:              (?, ?, 80)
  residual out:             (?, ?, 512)
  projected residual out:   (?, ?, 80)
  mel out:                  (?, ?, 80)
  linear out:               (?, ?, 1025)
  <stop_token> out:         (?, ?)
  Tacotron Parameters       29.016 Million.
Tacotron training set to a maximum of 100000 steps
No model to load at logs-Tacotron/taco_pretrained

Generated 20 test batches of size 32 in 112.255 sec

Generated 64 train batches of size 32 in 201.886 sec
Step       1 [231.909 sec/step, loss=24.10799, avg_loss=24.10799]
Saving Model Character Embeddings visualization..
Tacotron Character embeddings have been updated on tensorboard!
Exiting due to exception: OOM when allocating tensor with shape[32,154,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node Tacotron_model/inference/decoder/while/CustomDecoderStep/Location_Sensitive_Attention/add}} = Add[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:GPU:0"](Tacotron_model/inference/decoder/while/CustomDecoderStep/Location_Sensitive_Attention/add/Enter, Tacotron_model/inference/decoder/while/CustomDecoderStep/Location_Sensitive_Attention/ExpandDims)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[{{node Tacotron_model/clip_by_global_norm/mul_9/_1015}} = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_12920_Tacotron_model/clip_by_global_norm/mul_9", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


Caused by op 'Tacotron_model/inference/decoder/while/CustomDecoderStep/Location_Sensitive_Attention/add', defined at:
  File "train.py", line 138, in <module>
    main()
  File "train.py", line 128, in main
    tacotron_train(args, log_dir, hparams)
  File "/data2/xiaoyubei/code/Tacotron-2/tacotron/train.py", line 399, in tacotron_train
    return train(log_dir, args, hparams)
  File "/data2/xiaoyubei/code/Tacotron-2/tacotron/train.py", line 156, in train
    model, stats = model_train_mode(args, feeder, hparams, global_step)
  File "/data2/xiaoyubei/code/Tacotron-2/tacotron/train.py", line 87, in model_train_mode
    is_training=True, split_infos=feeder.split_infos)
  File "/data2/xiaoyubei/code/Tacotron-2/tacotron/models/tacotron.py", line 173, in initialize
    swap_memory=hp.tacotron_swap_with_cpu)
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py", line 322, in dynamic_decode
    swap_memory=swap_memory)
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 3274, in while_loop
    return_same_structure)
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2994, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2929, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py", line 265, in body
    decoder_finished) = decoder.step(time, inputs, state)
  File "/data2/xiaoyubei/code/Tacotron-2/tacotron/models/custom_decoder.py", line 118, in step
    (cell_outputs, stop_token), cell_state = self._cell(inputs, state)
  File "/data2/xiaoyubei/code/Tacotron-2/tacotron/models/Architecture_wrappers.py", line 192, in __call__
    prev_max_attentions=state.max_attentions)
  File "/data2/xiaoyubei/code/Tacotron-2/tacotron/models/attention.py", line 14, in _compute_attention
    cell_output, state=attention_state, prev_max_attentions=prev_max_attentions)
  File "/data2/xiaoyubei/code/Tacotron-2/tacotron/models/attention.py", line 199, in __call__
    energy = _location_sensitive_score(processed_query, processed_location_features, self.keys)
  File "/data2/xiaoyubei/code/Tacotron-2/tacotron/models/attention.py", line 70, in _location_sensitive_score
    return tf.reduce_sum(v_a * tf.tanh(W_keys + W_query + W_fil + b_a), [2])
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py", line 862, in binary_op_wrapper
    return func(x, y, name=name)
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py", line 301, in add
    "Add", x=x, y=y, name=name)
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 488, in new_func
    return func(*args, **kwargs)
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3272, in create_op
    op_def=op_def)
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[32,154,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node Tacotron_model/inference/decoder/while/CustomDecoderStep/Location_Sensitive_Attention/add}} = Add[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:GPU:0"](Tacotron_model/inference/decoder/while/CustomDecoderStep/Location_Sensitive_Attention/add/Enter, Tacotron_model/inference/decoder/while/CustomDecoderStep/Location_Sensitive_Attention/ExpandDims)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[{{node Tacotron_model/clip_by_global_norm/mul_9/_1015}} = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_12920_Tacotron_model/clip_by_global_norm/mul_9", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

Using TensorFlow backend.
Traceback (most recent call last):
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1292, in _do_call
    return fn(*args)
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1277, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1367, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[32,154,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node Tacotron_model/inference/decoder/while/CustomDecoderStep/Location_Sensitive_Attention/add}} = Add[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:GPU:0"](Tacotron_model/inference/decoder/while/CustomDecoderStep/Location_Sensitive_Attention/add/Enter, Tacotron_model/inference/decoder/while/CustomDecoderStep/Location_Sensitive_Attention/ExpandDims)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[{{node Tacotron_model/clip_by_global_norm/mul_9/_1015}} = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_12920_Tacotron_model/clip_by_global_norm/mul_9", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/data2/xiaoyubei/code/Tacotron-2/tacotron/train.py", line 225, in train
    step, loss, opt = sess.run([global_step, model.loss, model.optimize])
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 887, in run
    run_metadata_ptr)
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1286, in _do_run
    run_metadata)
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1308, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[32,154,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node Tacotron_model/inference/decoder/while/CustomDecoderStep/Location_Sensitive_Attention/add}} = Add[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:GPU:0"](Tacotron_model/inference/decoder/while/CustomDecoderStep/Location_Sensitive_Attention/add/Enter, Tacotron_model/inference/decoder/while/CustomDecoderStep/Location_Sensitive_Attention/ExpandDims)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[{{node Tacotron_model/clip_by_global_norm/mul_9/_1015}} = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_12920_Tacotron_model/clip_by_global_norm/mul_9", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


Caused by op 'Tacotron_model/inference/decoder/while/CustomDecoderStep/Location_Sensitive_Attention/add', defined at:
  File "train.py", line 138, in <module>
    main()
  File "train.py", line 128, in main
    tacotron_train(args, log_dir, hparams)
  File "/data2/xiaoyubei/code/Tacotron-2/tacotron/train.py", line 399, in tacotron_train
    return train(log_dir, args, hparams)
  File "/data2/xiaoyubei/code/Tacotron-2/tacotron/train.py", line 156, in train
    model, stats = model_train_mode(args, feeder, hparams, global_step)
  File "/data2/xiaoyubei/code/Tacotron-2/tacotron/train.py", line 87, in model_train_mode
    is_training=True, split_infos=feeder.split_infos)
  File "/data2/xiaoyubei/code/Tacotron-2/tacotron/models/tacotron.py", line 173, in initialize
    swap_memory=hp.tacotron_swap_with_cpu)
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py", line 322, in dynamic_decode
    swap_memory=swap_memory)
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 3274, in while_loop
    return_same_structure)
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2994, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2929, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py", line 265, in body
    decoder_finished) = decoder.step(time, inputs, state)
  File "/data2/xiaoyubei/code/Tacotron-2/tacotron/models/custom_decoder.py", line 118, in step
    (cell_outputs, stop_token), cell_state = self._cell(inputs, state)
  File "/data2/xiaoyubei/code/Tacotron-2/tacotron/models/Architecture_wrappers.py", line 192, in __call__
    prev_max_attentions=state.max_attentions)
  File "/data2/xiaoyubei/code/Tacotron-2/tacotron/models/attention.py", line 14, in _compute_attention
    cell_output, state=attention_state, prev_max_attentions=prev_max_attentions)
  File "/data2/xiaoyubei/code/Tacotron-2/tacotron/models/attention.py", line 199, in __call__
    energy = _location_sensitive_score(processed_query, processed_location_features, self.keys)
  File "/data2/xiaoyubei/code/Tacotron-2/tacotron/models/attention.py", line 70, in _location_sensitive_score
    return tf.reduce_sum(v_a * tf.tanh(W_keys + W_query + W_fil + b_a), [2])
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py", line 862, in binary_op_wrapper
    return func(x, y, name=name)
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py", line 301, in add
    "Add", x=x, y=y, name=name)
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 488, in new_func
    return func(*args, **kwargs)
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3272, in create_op
    op_def=op_def)
  File "/home/wubowen/.conda/envs/xyb_tac2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[32,154,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node Tacotron_model/inference/decoder/while/CustomDecoderStep/Location_Sensitive_Attention/add}} = Add[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:GPU:0"](Tacotron_model/inference/decoder/while/CustomDecoderStep/Location_Sensitive_Attention/add/Enter, Tacotron_model/inference/decoder/while/CustomDecoderStep/Location_Sensitive_Attention/ExpandDims)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[{{node Tacotron_model/clip_by_global_norm/mul_9/_1015}} = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_12920_Tacotron_model/clip_by_global_norm/mul_9", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.



Checkpoint path: logs-Tacotron/taco_pretrained/tacotron_model.ckpt
Loading training data from: training_data/train.txt
Using model: Tacotron
Hyperparameters:
  GL_on_GPU: True
  NN_init: True
  NN_scaler: 0.3
  allow_clipping_in_normalization: True
  attention_dim: 128
  attention_filters: 32
  attention_kernel: (31,)
  attention_win_size: 7
  batch_norm_position: after
  cbhg_conv_channels: 128
  cbhg_highway_units: 128
  cbhg_highwaynet_layers: 4
  cbhg_kernels: 8
  cbhg_pool_size: 2
  cbhg_projection: 256
  cbhg_projection_kernel_size: 3
  cbhg_rnn_units: 128
  cdf_loss: False
  cin_channels: 80
  cleaners: english_cleaners
  clip_for_wavenet: True
  clip_mels_length: True
  clip_outputs: True
  cross_entropy_pos_weight: 1
  cumulative_weights: True
  decoder_layers: 2
  decoder_lstm_units: 1024
  embedding_dim: 512
  enc_conv_channels: 512
  enc_conv_kernel_size: (5,)
  enc_conv_num_layers: 3
  encoder_lstm_units: 256
  fmax: 7600
  fmin: 55
  frame_shift_ms: None
  freq_axis_kernel_size: 3
  gate_channels: 256
  gin_channels: -1
  griffin_lim_iters: 60
  hop_size: 275
  input_type: raw
  kernel_size: 3
  layers: 20
  leaky_alpha: 0.4
  legacy: True
  log_scale_min: -32.23619130191664
  log_scale_min_gauss: -16.11809565095832
  lower_bound_decay: 0.1
  magnitude_power: 2.0
  mask_decoder: False
  mask_encoder: True
  max_abs_value: 4.0
  max_iters: 10000
  max_mel_frames: 900
  max_time_sec: None
  max_time_steps: 11000
  min_level_db: -100
  n_fft: 2048
  n_speakers: 5
  normalize_for_wavenet: True
  num_freq: 1025
  num_mels: 80
  out_channels: 2
  outputs_per_step: 1
  postnet_channels: 512
  postnet_kernel_size: (5,)
  postnet_num_layers: 5
  power: 1.5
  predict_linear: True
  preemphasis: 0.97
  preemphasize: True
  prenet_layers: [256, 256]
  quantize_channels: 65536
  ref_level_db: 20
  rescale: True
  rescaling_max: 0.999
  residual_channels: 128
  residual_legacy: True
  sample_rate: 22050
  signal_normalization: True
  silence_threshold: 2
  skip_out_channels: 128
  smoothing: False
  speakers: ['speaker0', 'speaker1', 'speaker2', 'speaker3', 'speaker4']
  speakers_path: None
  split_on_cpu: True
  stacks: 2
  stop_at_any: True
  symmetric_mels: True
  synthesis_constraint: False
  synthesis_constraint_type: window
  tacotron_adam_beta1: 0.9
  tacotron_adam_beta2: 0.999
  tacotron_adam_epsilon: 1e-06
  tacotron_batch_size: 32
  tacotron_clip_gradients: True
  tacotron_data_random_state: 1234
  tacotron_decay_learning_rate: True
  tacotron_decay_rate: 0.5
  tacotron_decay_steps: 18000
  tacotron_dropout_rate: 0.5
  tacotron_final_learning_rate: 0.0001
  tacotron_fine_tuning: False
  tacotron_initial_learning_rate: 0.001
  tacotron_natural_eval: False
  tacotron_num_gpus: 1
  tacotron_random_seed: 5339
  tacotron_reg_weight: 1e-06
  tacotron_scale_regularization: False
  tacotron_start_decay: 40000
  tacotron_swap_with_cpu: False
  tacotron_synthesis_batch_size: 1
  tacotron_teacher_forcing_decay_alpha: None
  tacotron_teacher_forcing_decay_steps: 40000
  tacotron_teacher_forcing_final_ratio: 0.0
  tacotron_teacher_forcing_init_ratio: 1.0
  tacotron_teacher_forcing_mode: constant
  tacotron_teacher_forcing_ratio: 1.0
  tacotron_teacher_forcing_start_decay: 10000
  tacotron_test_batches: None
  tacotron_test_size: 0.05
  tacotron_zoneout_rate: 0.1
  train_with_GTA: True
  trim_fft_size: 2048
  trim_hop_size: 512
  trim_silence: True
  trim_top_db: 40
  upsample_activation: Relu
  upsample_scales: [11, 25]
  upsample_type: SubPixel
  use_bias: True
  use_lws: False
  use_speaker_embedding: True
  wavenet_adam_beta1: 0.9
  wavenet_adam_beta2: 0.999
  wavenet_adam_epsilon: 1e-06
  wavenet_batch_size: 8
  wavenet_clip_gradients: True
  wavenet_data_random_state: 1234
  wavenet_debug_mels: ['training_data/mels/mel-LJ001-0008.npy']
  wavenet_debug_wavs: ['training_data/audio/audio-LJ001-0008.npy']
  wavenet_decay_rate: 0.5
  wavenet_decay_steps: 200000
  wavenet_dropout: 0.05
  wavenet_ema_decay: 0.9999
  wavenet_gradient_max_norm: 100.0
  wavenet_gradient_max_value: 5.0
  wavenet_init_scale: 1.0
  wavenet_learning_rate: 0.001
  wavenet_lr_schedule: exponential
  wavenet_natural_eval: False
  wavenet_num_gpus: 1
  wavenet_pad_sides: 1
  wavenet_random_seed: 5339
  wavenet_swap_with_cpu: False
  wavenet_synth_debug: False
  wavenet_synthesis_batch_size: 20
  wavenet_test_batches: 1
  wavenet_test_size: None
  wavenet_warmup: 4000.0
  wavenet_weight_normalization: False
  win_size: 1100
Loaded metadata for 13100 examples (23.76 hours)
initialisation done /gpu:0
Initialized Tacotron model. Dimensions (? = dynamic shape): 
  Train mode:               True
  Eval mode:                False
  GTA mode:                 False
  Synthesis mode:           False
  Input:                    (?, ?)
  device:                   0
  embedding:                (?, ?, 512)
  enc conv out:             (?, ?, 512)
  encoder out:              (?, ?, 512)
  decoder out:              (?, ?, 80)
  residual out:             (?, ?, 512)
  projected residual out:   (?, ?, 80)
  mel out:                  (?, ?, 80)
  linear out:               (?, ?, 1025)
  <stop_token> out:         (?, ?)
  Tacotron Parameters       29.016 Million.
initialisation done /gpu:0
Initialized Tacotron model. Dimensions (? = dynamic shape): 
  Train mode:               False
  Eval mode:                True
  GTA mode:                 False
  Synthesis mode:           False
  Input:                    (?, ?)
  device:                   0
  embedding:                (?, ?, 512)
  enc conv out:             (?, ?, 512)
  encoder out:              (?, ?, 512)
  decoder out:              (?, ?, 80)
  residual out:             (?, ?, 512)
  projected residual out:   (?, ?, 80)
  mel out:                  (?, ?, 80)
  linear out:               (?, ?, 1025)
  <stop_token> out:         (?, ?)
  Tacotron Parameters       29.016 Million.
Tacotron training set to a maximum of 100000 steps
Loading checkpoint logs-Tacotron/taco_pretrained/tacotron_model.ckpt-0

Generated 20 test batches of size 32 in 107.695 sec

Generated 64 train batches of size 32 in 172.508 sec
Step       1 [208.552 sec/step, loss=24.92592, avg_loss=24.92592]
Saving Model Character Embeddings visualization..
Tacotron Character embeddings have been updated on tensorboard!
Step       2 [112.226 sec/step, loss=17.33052, avg_loss=21.12822]Step       3 [81.798 sec/step, loss=11.36028, avg_loss=17.87224]Step       4 [64.564 sec/step, loss=11.13104, avg_loss=16.18694]Step       5 [55.449 sec/step, loss=10.03162, avg_loss=14.95588]Step       6 [48.995 sec/step, loss=9.64863, avg_loss=14.07134]Step       7 [45.171 sec/step, loss=8.71716, avg_loss=13.30645]Step       8 [42.115 sec/step, loss=8.86837, avg_loss=12.75169]Step       9 [39.132 sec/step, loss=8.28215, avg_loss=12.25508]Step      10 [36.812 sec/step, loss=8.25270, avg_loss=11.85484]Step      11 [34.697 sec/step, loss=7.96545, avg_loss=11.50126]Step      12 [32.629 sec/step, loss=8.55682, avg_loss=11.25589]Step      13 [30.921 sec/step, loss=7.85390, avg_loss=10.99420]Step      14 [30.322 sec/step, loss=7.47639, avg_loss=10.74292]Step      15 [29.668 sec/step, loss=7.10780, avg_loss=10.50058]Step      16 [28.107 sec/step, loss=8.47120, avg_loss=10.37375]Step      17 [27.684 sec/step, loss=6.75176, avg_loss=10.16069]Step      18 [27.064 sec/step, loss=6.73353, avg_loss=9.97029]Step      19 [26.368 sec/step, loss=6.62383, avg_loss=9.79416]Step      20 [25.525 sec/step, loss=6.63652, avg_loss=9.63628]Step      21 [24.905 sec/step, loss=6.38543, avg_loss=9.48148]Step      22 [24.557 sec/step, loss=6.15315, avg_loss=9.33019]Step      23 [23.864 sec/step, loss=6.36054, avg_loss=9.20107]Step      24 [23.749 sec/step, loss=5.55006, avg_loss=9.04895]Step      25 [23.630 sec/step, loss=5.62944, avg_loss=8.91217]Step      26 [23.364 sec/step, loss=5.49676, avg_loss=8.78081]Step      27 [22.915 sec/step, loss=5.79680, avg_loss=8.67029]Step      28 [22.834 sec/step, loss=5.32330, avg_loss=8.55075]Step      29 [22.510 sec/step, loss=5.31479, avg_loss=8.43917]Step      30 [22.017 sec/step, loss=5.62480, avg_loss=8.34535]Step      31 [21.909 sec/step, loss=5.03379, avg_loss=8.23853]